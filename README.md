# VLM4Bio: Species Classification Replication Project

A partial replication of the NeurIPS 2024 paper [VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images](https://proceedings.neurips.cc/paper/2024/file/...). This repository focuses on the **Species Classification** task, assessing the zero-shot performance of various openâ€‘source Visionâ€‘Language Models (VLMs) on fish, bird, and butterfly images.

## ğŸ“‹ Table of Contents

- [ğŸ” Introduction](#-introduction)
- [âœ¨ Features](#-features)
- [ğŸ“Š Dataset](#-dataset)
- [ğŸ§  Models](#-models)
- [ğŸ“ Project Structure](#-project-structure)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸš€ Usage](#-usage)
- [ğŸ§ª Experiments](#-experiments)
- [ğŸ“ˆ Results](#-results)
- [ğŸ“Š Presentation Summary](#-presentation-summary)
- [ğŸ›  Contributing](#-contributing)
- [âœ‰ï¸ Contact](#-contact)

---

## ğŸ” Introduction

Visionâ€‘Language Models (VLMs) have shown remarkable zeroâ€‘shot capabilities on generic vision tasks. **VLM4Bio** introduces a benchmark to evaluate these models on trait discovery within biological images. This replication project zeroes in on the Species Classification task, measuring how well pretrained VLMs can identify species across three taxonomic groups without any fineâ€‘tuning.

## âœ¨ Features

- **Zeroâ€‘Shot Evaluation**: Test pretrained models directly on unseen biological images.
- **Difficulty Tiers**: Split images into *Easy*, *Medium*, and *Hard* subsets to analyze performance across levels of visual challenge.
- **Prompt Engineering**: Compare different prompting strategies, including contextual prompts, dense captions, and Chainâ€‘ofâ€‘Thought (CoT).
- **Robustness Checks**: Perform Falseâ€‘Confidence Test (FCT) and "None of the Above" (NOTA) experiments to gauge model reliability.

## ğŸ“Š Dataset

Images and metadata are hosted on Kaggle:

- ğŸ“ [Image Dataset (10k images)](https://www.kaggle.com/datasets/aryamanporwal12/vlm4bio-10k-images)
- ğŸ§¾ [Metadata & CSV Files](https://www.kaggle.com/datasets/aryamanporwal12/vlm4bio-csv)

CSV files include:

- `fish.csv` â€” Groundâ€‘truth labels and image paths for fish species.
- `bird.csv` â€” Metadata for bird species classification.
- `butterfly.csv` â€” Metadata for butterfly species classification.

The CSVs contain the following columns:

| Column      | Description                                   |
|-------------|-----------------------------------------------|
| `image_path`| Local path to the image file                  |
| `question`  | Classification prompt/question                |
| `options`   | List of candidate species (for MC tasks)      |
| `answer`    | Groundâ€‘truth species label (zeroâ€‘shot target) |

## ğŸ§  Models

We evaluated the following pretrained VLMs (from Hugging Face):

| Model Alias                       | HF Model ID                         |
|-----------------------------------|-------------------------------------|
| BLIP2-Flan-T5                     | `Salesforce/blip2-flan-t5-xxl`      |
| BLIP-VQA-Base                     | `Salesforce/blip-vqa-base`          |
| Qwen2-VL-7B-Instruct              | `Qwen/Qwen2-VL-7B-Instruct`         |
| LLaVA-1.5-7B                      | `llava-hf/llava-1.5-7b-hf`          |
| Qwen2-VL-2B-Instruct              | `Qwen/Qwen2-VL-2B-Instruct`         |
| BLIP-VQA-CapFilt-Large            | `Salesforce/blip-vqa-capfilt-large` |

*Feel free to add or swap models by updating the `--model_name` argument.*

## ğŸ“ Project Structure

```text
VLM4Bio/
â”œâ”€â”€ data_for_easy_med_hard/       # Split images by difficulty
â”‚   â”œâ”€â”€ easy/
â”‚   â”œâ”€â”€ medium/
â”‚   â””â”€â”€ hard/
â”œâ”€â”€ bird.csv                      # Bird metadata
â”œâ”€â”€ butterfly.csv                 # Butterfly metadata
â”œâ”€â”€ fish.csv                      # Fish metadata
â”œâ”€â”€ main.py                       # Run experiments on easy/medium
â”œâ”€â”€ main_for_hard_data.py         # Run experiments on hard data
â”œâ”€â”€ merging_data.py               # Utilities to merge/preprocess CSVs
â”œâ”€â”€ results/                      # Output logs and metrics for exp1
â”œâ”€â”€ results_exp2/                 # Output logs and metrics for exp2
â”œâ”€â”€ species classification.pdf    # Detailed experiment report
â”œâ”€â”€ Project_Presentation final.pdf# Slide deck overview
â””â”€â”€ README.md                     # â† You are here
```
## âš™ï¸ Installation
```bash
#clone the repo
git clone https://github.com/Aryamanporwal/VLM4Bio.git
cd VLM4Bio

# create & activate venv
python3 -m venv .venv
source .venv/bin/activate

# install deps
pip install torch torchvision transformers pandas numpy scikit-learn matplotlib
```
(If a requirements.txt is available, you can instead run pip install -r requirements.txt.)

##ğŸš€ Usage
```bash
#Species Classification (Easy/Medium)
python main.py \
  --model_name Salesforce/blip2-flan-t5-xxl \
  --dataset easy
#--model_name: HF model identifier
#--dataset: easy, medium
#Species Classification (Hard)
python main_for_hard_data.py \
  --model_name llava-hf/llava-1.5-7b-hf
```
## ğŸ§ª Experiments

- **Zeroâ€‘Shot Accuracy**: Measures standard classification accuracy.
- **Prompt Ablation**: Compares contextual vs. CoT prompts.
- **Robustness Tests**: FCT and NOTA to test model confidence and outâ€‘ofâ€‘domain handling.

*Detailed methodologies are described in [species classification.pdf](species%20classification.pdf).*

## ğŸ“ˆ Results

- Raw logs and metric summaries are available under `results/` and `results_exp2/`.
- **Key findings**:
  - BLIP2-Flan-T5 achieved highest zeroâ€‘shot accuracy on *easy* images.
  - Performance degrades significantly on *hard* subset without CoT prompts.

## ğŸ“Š Presentation Summary

The **Project Presentation** (PDF) offers a visual and conceptual overview of the goals, methodologies, and key findings of the replication effort. It outlines:

- Motivation and relevance of biological trait discovery
- Description of the datasets and taxonomy splits
- Challenges in fine-grained species classification
- VLM architectures explored and their comparative performance
- Observations from experiments on difficulty tiers and prompt types

## ğŸ›  Contributing

Contributions are welcome! To add new models or evaluation protocols:

1. Fork the repository  
2. Create a feature branch (`git checkout -b feature/your-change`)  
3. Implement changes and update documentation  
4. Submit a Pull Request

Please follow the [Contributor Covenant Code of Conduct](https://www.contributor-covenant.org/).

## âœ‰ï¸ Contact

Maintainer: Aryaman Porwal  
Email: [aryamanlucknow@gmail.com](mailto:aryamanlucknow@gmail.com)

